{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(25,activation = 'relu'),\n",
    "        Dense(15,activation = 'relu'),\n",
    "        Dense(4,activation = 'softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.2328\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5190\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2218\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1259\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0909\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0728\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0599\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0467\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0427\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0364\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0317\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0265\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0238\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0221\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 832us/step - loss: 0.0197\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.0159\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 785us/step - loss: 0.0143\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 820us/step - loss: 0.0139\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 827us/step - loss: 0.0135\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 833us/step - loss: 0.0132\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 963us/step - loss: 0.0125\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 1000us/step - loss: 0.0119\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0111\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 975us/step - loss: 0.0108\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0108\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0109\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0100\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28caa4a3d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.04212125, -0.4416708 , -0.6634444 ,  0.58318794, -0.16471785,\n",
       "          0.6428728 , -0.54995453,  0.22343463, -0.04734604,  0.18710615,\n",
       "          0.5327076 ,  0.29225758, -0.08783607, -0.05770756, -0.21230231,\n",
       "         -0.16955695,  0.49266616,  0.04776525,  0.7688    ,  0.15065251,\n",
       "         -0.7320899 ,  0.44273305, -0.7812396 , -0.01753683,  0.45625508],\n",
       "        [ 0.14286828,  0.4363701 ,  0.3097809 ,  0.10978492, -0.57390076,\n",
       "         -0.1399857 ,  0.14245382,  0.14672627, -0.3183951 ,  0.44123605,\n",
       "          0.09695591, -0.43856907,  0.7075178 ,  0.62066907,  0.30381134,\n",
       "         -0.2154776 , -0.29002297, -0.29893526,  0.14239682,  0.17450167,\n",
       "         -0.35773364, -0.6239253 , -0.09804646, -0.62435305, -0.7871179 ]],\n",
       "       dtype=float32),\n",
       " array([ 0.6056893 , -0.4728431 , -0.54710543,  0.1641788 ,  0.6377889 ,\n",
       "         0.1874638 ,  0.25241888,  0.39135426,  0.4467215 ,  0.6929543 ,\n",
       "         0.15318303, -0.37296835,  0.0761995 ,  0.26551977, -0.41794598,\n",
       "         0.49976456, -0.39874282,  0.4625459 ,  0.21983698,  0.571752  ,\n",
       "         0.33845818, -0.4628811 ,  0.05751621,  0.42522988,  0.09973828],\n",
       "       dtype=float32),\n",
       " array([[ 3.92549455e-01,  2.92847335e-01,  5.72105110e-01,\n",
       "          3.49743128e-01, -2.93825686e-01, -1.61268134e-02,\n",
       "         -1.24137744e-01,  5.81905365e-01, -4.83607471e-01,\n",
       "          1.33910000e-01,  4.30715270e-02, -3.27041239e-01,\n",
       "         -2.92729288e-02,  5.63639224e-01, -1.04720980e-01],\n",
       "        [ 2.23439366e-01, -8.59781727e-02, -7.48842478e-01,\n",
       "         -4.83963013e-01, -3.52261871e-01, -1.57439988e-02,\n",
       "         -3.49709749e-01,  5.95010996e-01, -2.25110859e-01,\n",
       "         -8.16667732e-03,  6.24542534e-01, -6.61517903e-02,\n",
       "          4.28614877e-02, -3.49335581e-01, -8.79700929e-02],\n",
       "        [ 6.46318853e-01, -1.28057837e-01, -6.13198340e-01,\n",
       "         -3.70330736e-02,  2.02728882e-01, -2.22933833e-02,\n",
       "         -2.31022745e-01,  4.09718096e-01, -4.46615100e-01,\n",
       "          2.46351123e-01,  4.75923389e-01,  1.31271482e-01,\n",
       "          6.27867058e-02, -5.81344254e-02, -1.75845861e-01],\n",
       "        [ 7.30309114e-02, -3.09312977e-02, -1.12588286e-01,\n",
       "         -6.34967029e-01, -3.54225516e-01,  1.99348256e-01,\n",
       "         -9.91313815e-01,  4.67418469e-02,  5.47449291e-01,\n",
       "          1.89690828e-01,  1.20261297e-01, -1.55106619e-01,\n",
       "         -9.14098680e-01,  1.35628518e-03,  6.77938223e-01],\n",
       "        [-5.73844850e-01,  4.09338474e-02,  4.69073057e-01,\n",
       "         -1.46817803e-01, -2.42703602e-01, -4.44471627e-01,\n",
       "          5.08277297e-01,  5.58833592e-03, -1.55700728e-01,\n",
       "         -3.16655040e-01, -8.48589092e-02, -3.13281000e-01,\n",
       "         -1.18381932e-01,  1.57524362e-01, -2.70246625e-01],\n",
       "        [ 8.03626329e-02, -2.27765843e-01,  3.27218398e-02,\n",
       "         -2.67837524e-01, -8.25033039e-02,  6.29983783e-01,\n",
       "         -2.32124239e-01,  1.95353910e-01,  3.89804870e-01,\n",
       "         -3.47864591e-02, -4.07763720e-01, -2.80824214e-01,\n",
       "          1.34135976e-01, -3.71994674e-01,  2.33112201e-01],\n",
       "        [ 4.05394435e-01,  1.12401009e-01,  7.11080013e-03,\n",
       "          1.84098467e-01, -1.44378528e-01, -2.63766646e-01,\n",
       "          3.15946817e-01,  1.58492073e-01, -2.19215423e-01,\n",
       "         -3.83147858e-02,  2.56265432e-01, -6.26805145e-03,\n",
       "          4.24956471e-01, -4.40508157e-01,  1.81167811e-01],\n",
       "        [-3.40843290e-01, -1.26265362e-02,  3.66000205e-01,\n",
       "         -2.18144938e-01, -2.06536442e-01,  7.14440525e-01,\n",
       "         -6.54111654e-02, -4.04171757e-02,  8.57754871e-02,\n",
       "          1.79243430e-01, -8.80198479e-01, -8.78985301e-02,\n",
       "         -7.01344460e-02, -3.17695141e-01,  1.03713579e-01],\n",
       "        [ 1.51575178e-01, -1.24232471e-02,  6.81954205e-01,\n",
       "          1.52425513e-01, -4.12849516e-01, -3.17625433e-01,\n",
       "          8.90679881e-02,  1.28161877e-01,  3.17848057e-01,\n",
       "         -2.02132329e-01, -2.42392302e-01,  9.04750545e-03,\n",
       "          2.56386071e-01,  1.78839654e-01, -2.93238796e-02],\n",
       "        [-1.20400988e-01, -1.61724433e-01,  4.29950088e-01,\n",
       "          3.69567424e-01, -3.61096188e-02,  2.44471699e-01,\n",
       "          3.81574005e-01, -1.42105296e-02, -3.78480852e-02,\n",
       "          2.52540886e-01, -8.45937282e-02, -1.23790823e-01,\n",
       "          3.16056371e-01,  5.91151059e-01,  1.72698781e-01],\n",
       "        [-4.06012014e-02,  2.79234350e-01,  5.33816870e-03,\n",
       "         -1.22209024e+00,  2.73418963e-01,  4.51522350e-01,\n",
       "         -1.25934911e+00, -1.02691524e-01,  3.93468589e-01,\n",
       "         -4.51924980e-01, -5.38172483e-01,  1.64840966e-01,\n",
       "         -8.87767673e-01,  2.11521685e-01,  2.26723224e-01],\n",
       "        [-3.91139567e-01,  4.16973978e-02, -6.20014310e-01,\n",
       "          1.55207127e-01,  1.55935422e-01, -1.74689800e-01,\n",
       "         -1.17631942e-01, -3.39839697e-01,  5.54633737e-01,\n",
       "         -5.89160025e-01, -2.84115840e-02,  1.09403610e-01,\n",
       "         -2.37392192e-03, -1.86651871e-01,  1.40769482e-01],\n",
       "        [ 3.82119238e-01, -8.98013934e-02,  1.70804009e-01,\n",
       "          3.01913898e-02, -8.32045302e-02,  5.03287971e-01,\n",
       "         -8.12039673e-02,  4.44821805e-01, -5.73311746e-01,\n",
       "          5.57352602e-01,  6.21843994e-01, -1.60677224e-01,\n",
       "         -1.79368943e-01,  4.75004077e-01,  2.11611241e-01],\n",
       "        [-2.68327259e-03, -3.24515641e-01,  2.83952802e-01,\n",
       "         -1.64081991e-01, -3.58652264e-01,  4.34935272e-01,\n",
       "         -5.59477694e-02,  3.51387292e-01, -1.74728513e-01,\n",
       "          3.19171995e-01, -2.00183421e-01,  2.33702123e-01,\n",
       "         -2.50091553e-01,  9.36225578e-02, -1.84384733e-02],\n",
       "        [ 5.44778049e-01, -1.21739976e-01, -5.50107300e-01,\n",
       "         -5.09729385e-01,  1.95955470e-01,  4.40300629e-02,\n",
       "         -4.59706932e-01,  2.82671154e-01, -8.25478792e-01,\n",
       "          2.56176651e-01,  2.74718344e-01,  2.27424577e-01,\n",
       "         -4.83627856e-01, -3.18553120e-01, -1.26889795e-01],\n",
       "        [-8.92454982e-02,  1.85291469e-01,  5.28444111e-01,\n",
       "          3.67653012e-01, -2.34370992e-01, -5.08773565e-01,\n",
       "          4.00651395e-01,  1.64940059e-01,  5.95556617e-01,\n",
       "         -1.89192370e-02,  1.49969101e-01, -1.55136004e-01,\n",
       "          5.61491787e-01, -1.76550090e-01, -3.78330827e-01],\n",
       "        [ 2.45403603e-01, -4.06710625e-01, -7.24572122e-01,\n",
       "          7.87121058e-01,  2.56372839e-01, -2.98321247e-01,\n",
       "         -2.62991674e-02, -1.45559072e-01,  2.35906824e-01,\n",
       "         -1.89480856e-01,  2.25158393e-01,  2.90633917e-01,\n",
       "          3.87262583e-01, -2.39140302e-01,  8.39026794e-02],\n",
       "        [-7.34680772e-01, -2.30823517e-01,  5.60197175e-01,\n",
       "          1.12578169e-01, -8.12061224e-03,  2.64078647e-01,\n",
       "          7.82558843e-02,  1.95550188e-01,  2.27127507e-01,\n",
       "         -4.52053279e-01, -1.64160039e-02, -3.89167190e-01,\n",
       "          4.11454260e-01, -3.20873499e-01,  5.00014007e-01],\n",
       "        [-5.57284243e-02,  3.37758899e-01, -2.35881150e-01,\n",
       "         -9.24852669e-01, -1.48957551e-01,  9.69968140e-02,\n",
       "         -1.07517338e+00, -3.48028727e-03,  2.89682060e-01,\n",
       "          2.54623592e-01,  1.88376844e-01, -3.24588984e-01,\n",
       "         -4.11329776e-01,  1.45790458e-01,  5.56354403e-01],\n",
       "        [-6.26319945e-01, -3.95837903e-01,  4.53014255e-01,\n",
       "          4.23186809e-01,  1.03916965e-01,  5.96742332e-01,\n",
       "         -1.26268402e-01,  2.16975138e-01, -2.52628118e-01,\n",
       "          4.63557631e-01, -9.01352763e-01, -3.57213527e-01,\n",
       "         -9.16442424e-02,  7.77871490e-01,  3.13919038e-01],\n",
       "        [ 3.88068140e-01, -1.91749021e-01,  2.32281879e-01,\n",
       "          3.00829142e-01,  6.06841929e-02, -8.66944313e-01,\n",
       "          5.16602337e-01, -1.33380383e-01,  3.09442252e-01,\n",
       "          6.85896054e-02,  4.47567344e-01, -7.48255849e-03,\n",
       "          3.35034281e-01,  1.78075716e-01,  8.08659643e-02],\n",
       "        [ 8.46164767e-03,  2.34154090e-01, -2.22398654e-01,\n",
       "          1.40921915e+00,  1.32089943e-01, -4.49183702e-01,\n",
       "          6.53749555e-02, -3.73557419e-01,  6.53327644e-01,\n",
       "         -3.57362218e-02, -3.80677581e-01,  1.77235082e-01,\n",
       "          1.13492392e-01, -4.44487780e-01,  7.01906383e-02],\n",
       "        [ 5.81336737e-01, -7.30157793e-02, -3.80762666e-01,\n",
       "         -3.49288166e-01,  1.31000921e-01, -5.08306086e-01,\n",
       "          1.35054231e-01, -8.15880150e-02,  4.37411070e-01,\n",
       "          3.86049226e-02,  4.34729815e-01, -3.45565528e-02,\n",
       "          5.72794497e-01, -4.41775829e-01, -2.67241716e-01],\n",
       "        [-1.94183633e-01, -2.41523892e-01,  6.41461968e-01,\n",
       "         -1.62450597e-02,  2.78468072e-01,  3.60973924e-01,\n",
       "          4.57245380e-01,  2.47219518e-01,  1.18174866e-01,\n",
       "         -3.24482545e-02,  1.50696427e-01, -3.64310384e-01,\n",
       "          5.74320070e-02,  4.07221407e-01,  2.10736394e-01],\n",
       "        [ 1.97305277e-01, -2.32139885e-01, -3.14380556e-01,\n",
       "         -2.16477454e-01, -4.21964169e-01,  4.82168078e-01,\n",
       "          3.38546753e-01,  5.58258593e-03,  6.33221388e-01,\n",
       "         -4.44987088e-01, -3.17237228e-01,  1.87996864e-01,\n",
       "         -2.40347683e-01,  2.05758244e-01,  7.18944013e-01]], dtype=float32),\n",
       " array([-0.3090051 , -0.03922524,  0.9684359 ,  0.21918796, -0.0360022 ,\n",
       "         0.2720763 ,  0.28751475,  0.13420007,  0.06110528,  0.07435626,\n",
       "        -0.3729583 , -0.05862126,  0.28972855,  0.28363192,  0.16013408],\n",
       "       dtype=float32),\n",
       " array([[ 0.8739744 , -0.27602133, -0.3011276 , -0.13439167],\n",
       "        [-0.32283047,  0.05853167, -0.26934236, -0.50884557],\n",
       "        [-1.6383702 ,  0.6246308 ,  0.7188587 , -1.8604581 ],\n",
       "        [-1.021676  ,  0.17673384, -0.6045106 , -0.16569991],\n",
       "        [ 0.3247436 , -0.11777107, -0.41509005, -0.2798974 ],\n",
       "        [-0.5135718 , -0.9085173 ,  0.71964353,  0.45471567],\n",
       "        [ 0.00937567,  0.7954553 , -0.12915373, -0.0316698 ],\n",
       "        [ 0.34254   , -0.1268271 ,  0.32500958, -0.5087018 ],\n",
       "        [-0.41305336,  0.16221134, -0.42101002,  0.31278548],\n",
       "        [ 0.16309719, -0.3300572 ,  0.3583798 , -0.7162412 ],\n",
       "        [ 0.4109991 , -0.14825118, -0.425481  ,  0.05702156],\n",
       "        [ 0.32610697, -0.41642448,  0.16827478, -0.14280255],\n",
       "        [ 0.07633771,  0.63582534, -0.42136982, -0.6610334 ],\n",
       "        [ 0.05083968, -0.14291401,  0.92847276, -0.43448323],\n",
       "        [-0.56162536, -1.097953  , -0.17836837,  0.43255812]],\n",
       "       dtype=float32),\n",
       " array([-0.2778062 ,  0.22547494,  0.24745058, -0.1311726 ], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
